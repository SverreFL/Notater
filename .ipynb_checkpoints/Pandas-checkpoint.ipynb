{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datastrukturer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas bygger på numpy arrays og arver mye funksjonalitet derfra. Det er to sentrale objekt: Series og DataFrame. DataFrame kan betraktes som en tabell og Series som en kolonne fra en tabell.\n",
    "\n",
    "I tillegg til den implisitte numeriske indexen til arrays har de også et eksplisitt indexobjekt. I Series mapper verdi av index til enkeltverdi. I DataFrame mapper det til et Series-objekt. \n",
    "\n",
    "Koblingen mellom indeks og verdi gjør at vi kan kombinere data fra ulike tabeller, håndtere *missing values* og generelt ikke er avhengig av å ha helt konforme data (samme størrelse, samme rekkefølge) for å gjøre regneoperasjoner ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series er crossover mellom array og dict. Består av to arrays: én index og én med homogene data. Vanlige ndarrays har implisitt index med posisjon, men her er det eksplisitt og kan ha ulike labels. Kan også tenke på Series som en dict som mapper index-verdier til verdi i array. Følgelig er det en del metoder/syntax som tilsvarer dict. Kan få ut hver av arraysene med:\n",
    "1. a.values (ndarray)\n",
    "2. a.index (index-objekt, finnes litt ulike typer, har diverse metoder, noe mengdegreier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame er tabulær datastruktur med både rekke-index og kolonne-index. I utgangspunktet er det ganske symmetrisk, men av konvensjon angir rekke-index obseravjon og kolonne-index er variabel (dimensjon/egenskap) ved observasjonen. Kan betrakte det som en dict som mapper label til Series, der labels er enten fra rekke- eller kolonneindex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det er en fullverdi datastruktur/objekt i seg selv. Har en del metoder og sånn. Vet ikke om jeg må jobbe så mye direkte med indexen; kan gjøre operasjon på dataframe og så håndterer de index internt.\n",
    "\n",
    "\n",
    "Merk at når vi setter en ny kolonne som indeks mister vi den eksisterende index-kolonnnen. Hvis vi vil beholde må vi lagre kopi av kolonne og deretter putte den inn i nye dataframe. Kan få tak i kolonnen ved å bruke df.index() og deretter dytte den inn i df ved å assigne kolonnen med df[\"navn\"] = df.index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indeksing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan legge til verdier på subset av dataframe\n",
    "\n",
    "```python\n",
    "df.loc['a'] = arr\n",
    "df.loc['a'] = df_sub # fungerer ikke selvom matchene index, vet ikke hvorfor ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kolonner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan merke at kolonnene egentlig bare er index og at det i utgangspuntket er ganske symmetrisk.\n",
    "\n",
    "Vil ha deskriptive kolonnenavn. For å endre bruker vi dict med mapping\n",
    "```python\n",
    "mapper = {'gammel':'ny'}\n",
    "df = df.rename(columns=mapper)\n",
    "```\n",
    "Kan bruke vanlig list comprehension til å finne subset av kolonner som oppfyller kriterie,\n",
    "```python\n",
    "cols_subset = [col for col in df.columns if 'string' in col]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index med flere level. Ytterste er level=0, og så teller vi oppover "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Konstruere multiindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan eksplisitt konstruere multiindex objekt på flere måter\n",
    "\n",
    "```python\n",
    "arrays = [np.array([\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"]),\n",
    "          np.array([\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"])]\n",
    "\n",
    "# Tre ekvivalente indekser:\n",
    "pd.MultiIndex.from_tuples(zip(arrays[0],arrays[1])) # trenger list of tuples\n",
    "pd.MultiIndex.from_arrays(arrays) # trenger list of arrays\n",
    "pd.MultiIndex.from_frame(pd.DataFrame(arrays).T) # organisere arrays i dataframe før vi lage index\n",
    "\n",
    "pd.MultiIndex.from_product(iterables) # alle unike kombinasjoner av verdier i ulike arrays\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan også konstruere MultiIndex i constructor for DataFrame, må da være list of arrays\n",
    "```python\n",
    "df = pd.DataFrame(index=arrays, data=np.random.randn(len(index)), columns=['tall'])\n",
    "df.set_index('tall', append=True) # legge til kolonne på eksisterende index\n",
    "\n",
    "# Hvis vi ikke har eksisterne index kan vi lage nye multiindex med:\n",
    "df.set_index([\"kol1\",\"kol2\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis vi har lyst til å legge ny array (eller eventuelt konstant verdi) som øvereste level i multiindex bør vi transformere index til dataframe så den  blir enklere å manipulere\n",
    "```python\n",
    "temp = df.index.to_frame()\n",
    "temp.insert(0, 'name', value)\n",
    "df.index = pd.MultiIndex.from_frame(temp)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Slice multiindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsetting av observasjoner avhengig av verdier i multiindex kan bli ganske komplisert.\n",
    "```python\n",
    "\n",
    "df.loc['a'] # alle rekker der index i level 0 == 'a'\n",
    "df.loc[('a', 'b')] # alle rekker der level 0 == 'a' og level 1 == 'b'\n",
    "```\n",
    "\n",
    "Må se mer på dette senere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sortering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan sortere entent etter index eller verdi langs gitt kolonne,\n",
    "```python\n",
    "df.sort_index()\n",
    "df.sort_values(['index','kol'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lage dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan enten initialisere Series og DataFrame fra filer (lokalt på disk eller i sky) eller fra andre objekter i Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fra fil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De to vanlige lagringsformatene for tabulære data er .csv og .json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fra csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tekstfil der rader er separert på ulike linjer `\\n` og verdier er separerert med delimiter (gjerne `,` eller `;`). Siden det er en tekstfil kan vi åpne den i notepad for å inspisere strukturen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "pd.read_csv(filename,\n",
    "            sep=',', # kan bruke andre seps, tror tab er '\\t'\n",
    "            delimiter=None,\n",
    "            header='infer', # må spesifisere header=0 for å endre navn..\n",
    "            names=None, # hvis vi vil spesisifisere navn på kolonne\n",
    "            index_col=None, # hvis første kol skal være index bruker vi =0\n",
    "            usecols=None, # hvis vi vil spesifisere subset av kolonner\n",
    "            parse_dates=True, # finner kolonne med date, gjør den til index, får den i datetime.\n",
    "            skiprows=2, # hoppe over rader\n",
    "            dtype = {'name':'dtype'}, # tilsvarer å calle df.name.astype('dtype') ex-post\n",
    "            converters = {'name':func}, # tilsvarer df.name.apply(func) ex-post\n",
    "            ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fra json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det andre vanlige lagringsformatet for tabulære data er json (javascript object notation). Det korresponderer i stor grad med dictionary i Python. Må spesifisere `orient`\n",
    "1. split\n",
    "2. records\n",
    "3. columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fra datastrukturer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan også initialisere fra andre objekter i Python som representerer data (dictionary og arrays).\n",
    "```Python\n",
    "pd.Series(data=x,index=y). \n",
    "pd.Series(dict(zip(y,x))\n",
    "```          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagre dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan lagre dataframes som består av tall og strenger som tekst i csv format med `to_csv`\n",
    "\n",
    "Hvis det inneholder andre objekter, så må vi lagre til `to_pickle`. Generelt vil pickle lagre fullstendig informasjon om dataframe; alt fra datatyper, form av indeks, ... alt. Når vi lagrer til csv så er det bare en lang string og vi må tolke denne stringen når vi laster dataframe, så da starter vi gjerne fra scratch. Pickle er bedre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beskrive data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Før jeg kan gjøre noe analyse må jeg først undersøke tabellen med tall jeg har fått utdelt. Må få oversikt over:\n",
    "1. Hvilke egensakper (kolonner) og hva de måler\n",
    "2. Hva slags måleenhet de er i og konvertere til riktig datatype. I algebraen under the hood er alt tall, men representer som factor for å få bedre representasjon og utnytte funksjonalitet i plotte-libraries og lignende. Vil unngå å jobbe eksplisitt med dummies før det er nødvendig.\n",
    "3. Få noe oversikt over univariate fordelinger og eventuelt parvis bivariate korrelasjoner. Se at tall er rimelig, blir kjent med data, oppdage numerisk kodete missing values, se etter feilkodinger\n",
    "4. Få oversikt over manglende verdier, vurdere strategi for imputation og eventuelt bias dersom vi ser på delutvalg med fullstendig obervasjon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "df.describe() # output avhenger av datatype\n",
    "df.info() # datatype og observere om det er missing values\n",
    "df.ndtype() # finne datatypene\n",
    "df['col'].value_count(normalize=False) # antallet observasjoner med gitte verdier langs den dimensjonen, eller andel\n",
    "df['col'].unique() # liste av unike verdier\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datatyper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hver kolonne har en består av verdier med homogen datatype. Hvordan verdiene blir representert og hvilke funksjonaliterer som er gyldige avhenger av datatype, og det er derfor viktig å ha oversikt over dette. Pandas forsøker å tolke datatype hvis det ikke er eksplisitt spesifisert. Hvis ikke homogen representasjon vil den forsøke å konvertere verdier (f.eks. float og int blir til float).\n",
    "\n",
    "Vil håndtere spesifikasjon av datatyper gjennom `read_csv` for å separere det fra analysen. Kan være nødvendig med custom funksjon som \n",
    "```python\n",
    "def convert_currency(val):\n",
    "    return float(val.replace(',','').replace('$',''))\n",
    "df['col'] = df['col'].apply(convert_currency)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerisk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan enten være `int` (heltall) eller `float`. Kan spesifisere hvor mange bits vi vil sette av til å representere hver av tallene. Høyere bits betyr mer lagringsplass, men kan representere høyere tallverdier.\n",
    "\n",
    "Numeriske kolonner med manglende verdier blir konvertert til float. Kan ha lyst til å representere dette som ints...\n",
    "\n",
    "Har hjelpefunksjon for å konvertere til numerisk når .astype() ikke klarer å tolke\n",
    "```python\n",
    "pd.to_numeric(col, errors='coerce') # nans for verdier som ikke kan tolkes som numerisk\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kategorisk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tror det korresponderer med `factor` i `R`. Tror vi trenger representasjonen i statistisk analyse.. Liste av teksverdier.\n",
    "```python\n",
    "pd.Categorical(df.col, ordered=True, categories = [..]) # spesifisere rekkefølge hvis ordinal\n",
    "\n",
    "# Konvertere object til category\n",
    "cat_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
    "df[cat_cols] = df[cat_cols].astype('category')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objekt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strings eller mixed type .. Tror hvis mixed så har ikke verdiene homogen datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### String"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis kolonne er string kan jeg bruke df.col.str for å få tilgang til metoder. Dette er ofte bedre å raskere enn å gjøre en eksplisitt loop og bruke string metode på innholdet i hver celle. Har de fleste (alle?) metodene som native python har på strings. Har i tillegg noen egne funskjonaliteter knyttet til pandas, feks:\n",
    "```python\n",
    "# lage dummmies der hver observasjon kan inneholde flere verdier i form av string\n",
    "df['col'].str.get_dummies(sep='|') \n",
    "\n",
    "# Hvis jeg vil bruke flere str operasjoner i chain må jeg bruke .str etter hver, eks\n",
    "data['Min_Salary']=data['Min_Salary'].str.strip(' ').str.lstrip('$').str.rstrip('K').fillna(0).astype('int')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dato og tid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidspunkt har mange egenskaper. De har rangering der større verdier er senere enn små verdier, men det er langt fra tilstrekkelig å representere denne ordinale rangeringen med tall. Det finnes mange ulike målenheter på avstand i tid (dager, sekunder, millisekunder, mm.) og det er ikke alltid trivielt å konvertere mellom disse, samt at det avhenger av tidssone. Dessuten er det mye informasjon assosiert med gitte tidspunkt (hvilken uke eller dag det er, mm).\n",
    "\n",
    "Dersom vi gir informasjon om tidspunkt en riktig representasjon kan pandas gjøre mye arbeid for oss. Det gir oss tilgang til informasjon og det kan tolke hva vi mener slik at det blir enkelt å filtrere observasjoner ut fra årstall, ukedag eller lignende. Det kan også tolke avstand mellom tidspunkt i ulike måleenheter. For å oppnå dette må vi bruke classer som er *time aware*.\n",
    "\n",
    "Av uvisse grunner har det mer intuitiv funksjonalitet når datetime array er indeks i stedet for kolonne i dataframe... Gir vel uansett stort sett mening å ha det som index.\n",
    "\n",
    "Har scalar-class og array-class (eks: timestamp og datetimeindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representerer et punkt i tid. Ganske analog til datetime i base python og sql. Har hjelpefunksjon for å konvertere string-representasjon til timestamp.\n",
    "```python\n",
    "pd.to_datetime(array, # prøver å parse innhold i array, men vi må ofte i praksis hjelpe litt til med flere argument\n",
    "               format, # spesifisere format, litt usikker på hvordan... eks '%Y%m%d' for '2010/11/12'\n",
    "               day_first, # alternativt kan vi gi litt tips\n",
    "               infer_datatime_format=True, # prøver å gjette fra første ikke-nan. Kan være ambiguøst, så forsiktig ..\n",
    "               )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan konstruere index som består av datetime med \n",
    "```python\n",
    "date_index = pd.date_range(start, # string som kan bli parset som dato..\n",
    "                           period, # int med antall perioder\n",
    "                           freq, # avstand mellom perioder, 'D' for daily, 'H' hourly, ..\n",
    "                           end, # kan alternativt spesifisere sluttdato\n",
    "                           tz # kan spesifisere timezone. Kan deretter konverte med .tz_convert('time zone')\n",
    "                           )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bruker .dt til å få interface til egenskaper for datetime objekt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differanse i tid mellom to timestamps. Kan være resultat av artimetikk på timestamps..\n",
    "```python\n",
    "pd.Timedelta('3 days')\n",
    "pd.to_timedelta('3 days')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representerer intervallet av tidspunkt mellom to tidspunkt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi vil representere manglende verdier. Default er `pd.NaN` og og `np.NaT`. `np.NaN` er en float, slik at numeriske kolonner med missing values blir recast til floats. Det er lagt til et alternativ objekt `pd.NA` som representere atomisk verdi og tar sikte på å være uavhengig av datatype. Kan bruke i nullable integer,\n",
    "```python\n",
    "df['col']=df['col'].astype('Int64') # stor bokstav\n",
    "```\n",
    "Missing values propapegerer når vi gjør operasjoner på dataframe. Operasjon er elementvis og alt som bruker NaN resultererr i NaN. Aggregeringsfunksjoner pleier å ignorere NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Første jeg må gjøre er å sjekke antallet missing values i ulike kolonner. Husk at missing values kan være kodet på ulike måter i ulike datasett. En mulighet er å bruke df['col'].unique() til å se på verdier og se om noen ikke gir mening. Deretter vil jeg omkode de til NaN, f.eks. ved df.replace(value,np.nan). Når jeg har riktig koding kan jeg konstruere boolean mask for å identfisere\n",
    "```python\n",
    "df.isna() # gir boolean mask over hele dataframe\n",
    "df.isna().any() # angir om hvorvidt det er nans i hver av kolonner\n",
    "df.isna().sum() # antallet nans i hver kolonne\n",
    "\n",
    "# \n",
    "df.dropna(axis, # drop rad (0) eller kolonne (1) avhengig om missing i celle\n",
    "          how, # {'any', 'all'} # drop hvis noen missing eller alle missing langst angitt akse\n",
    "          ..)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En alternativ fremgangsmåte er å fylle inn verdier. Algoritmer i sklearn kan ikke håndtere nans og det er synd å kaste bort data bare fordi noen observasjoner er litt mangefulle. Finnes ulike strategier for dette som jeg kan se på senere\n",
    "1. Bruke gjennomsnitt i kolonnen\n",
    "2. Predikere verdi ut fra andre dimensjoner vi observere\n",
    "3. Bruke verdi fra naboer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split-apply-combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi har et stort datasett. Det er mange variabler og mange kolonner. Hvordan skal vi få ut noen innsikter fra all denne informasjonen?\n",
    "\n",
    "Kan vært veldig nyttig å dele det inn i mindre deler etter felles verdi langs en (eller flere) kolonne(r). Kan tenke at det er observasjoner som \"hører sammen\". Deretter annvender vi noen aggregeringsfunksjoner som gir oss slags sammendragsmål for tallverdiene i hver del. Deretter kan vi kombinere det sammen i nytt datasett som mapper nøkkel (unike(e) verdi(er) av identifiserende kolonne(r)) til verdi av aggregeringsfunksjon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupby-objekt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "df.groupby(by, # kolonnenavn eller liste av kolonnenavn som vi grupperer etter. Eller pd.Grouper..\n",
    "           level, # hvis vi vil bruke kolonner fra multiindex\n",
    "           as_index, # brukt som index i output.. kan sette false for å sikre at dataframe\n",
    "           observed # kan brukes hvis grupperingskolonne er `Categoricals` .. tror ikke så relevant\n",
    "           )\n",
    "```\n",
    "Det er en måte å konstruere separate dataframes filtrert på verdi av kolonne vi grupper etter. Kan få tak i de interne dataframene med\n",
    "```python\n",
    "gb = df.groupby('col')\n",
    "for idx, df in gb:\n",
    "    print(idx) # verdi av col\n",
    "    print(df) # tilsvarer df[df['col'==val]] for hver unik val i 'col'\n",
    "```    \n",
    "I praksis vil vi ikke jobbe direkte med tabellene internt i groupby-objektet, men i stedet bruke metoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan bruke hvis vi vil kjøre ulike aggegreringsfunksjon på ulike kolonner\n",
    "```python\n",
    "df_out = (df.groupby(\"key\").\n",
    "              agg({\"col1\":'mean', 'col2':'count'}).\n",
    "              rename(columns={'col1':'avg_col1', 'col2':'num_col2'}))\n",
    "```\n",
    "Det ser veldig stygt ut, så vi bruker i stedet `NamedAgg` objekt,\n",
    "```python\n",
    "df_out = df.groupby(\"key\").agg(\n",
    "    avg_col1 = pd.NamedAgg('col1', 'mean'),\n",
    "    num_col2 = pd.NamedAgg('col2', 'count'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-level groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "df_out = df.groupby(['first_column', 'second_column'])['some_column'].mean()\n",
    "```\n",
    "Merk at df_out bare vil ha index for de kombinasjoner av verdier i kolonnene vi grupperer på som vi faktisk observerer i data. I mange tilfeller kan det være greit å ha alle kombinasjonene og padde med NaNs for de vi ikke observerer. Litt usikker på hvordan jeg skal gjøre dette... må resette/konstruere denne indeksen på en måte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis vi har lyst til å gruppere etter verdier på en kolonne, men den har for fin inndeling (for mange unike verdier). Da kan vi bruke `pd.Grouper` til å lage en grovere inndeling ved å lag bins med flere verdier og gruppere sammen alle radene som korresponderer med samme bins. Brukes i praksis på kolonner med tidsdimensjon.\n",
    "\n",
    "```python\n",
    "# gruppere observasjoner med timedelta innefor 5 minutters intervall\n",
    "duration_agg = df.groupby(pd.Grouper(key='duration',freq='5Min'))['started_at'].agg('count')\n",
    "\n",
    "# Kan kombinere med annen key for å få gjennomsnitt av størrelse som varierer over tid innad i hver kategori..\n",
    "trips.groupby(['name', (pd.Grouper(key='start_ts',freq='D'))]).car_id.nunique().groupby('name').mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan tenke at kombinasjonen av en index-verdi og kolonne-verdi til sammen utgjør en tuple som identifiserer en celle med verdi i tabellen. Vi kan flytte ting mellom index og kolonne og likevel identifisere de samme cellene med verdier, men tabellen vil se ganske annerledes ut.\n",
    "\n",
    "For analyser og visualiseringer vil vi ha data på *tidy*-format der hver rad er en observasjon og hver kolonne en egenskap ved denne observasjonen.\n",
    "\n",
    "Tror ikke det er noen entydig definisjon på *wide* og *long*, men i long flytter vi identifikatorer ned til index og i wide så går det opp i liste av kolonnenavn..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Har to high-level funksjoner. Pivot som gjør *wide* og melt som gjøre *long* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "df.pivot(index, # hver unik verdi i kolonnen utgjør én rekke i ny df\n",
    "         columns, # hver unik verdi utgjør navn på én kolonne i ny df\n",
    "         values=, # hvilke kolonner som fyller verdiene i celler\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eksempel\n",
    "```python\n",
    "df = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two',\n",
    "                           'two'],\n",
    "                   'bar': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
    "                   'baz': [1, 2, 3, 4, 5, 6],\n",
    "                   'zoo': ['x', 'y', 'z', 'q', 'w', 't']})\n",
    "df.pivot(columns='foo', # kolonner er ['one', 'two']\n",
    "         index='bar', # index er ['A', 'B', 'C']\n",
    "         values='baz') # fyller inn verdi fra baz som korresponderer med verdiene av (foo, bar)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pivot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalisering som kan håndtere duplikat av (index, col)... Aggregering ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Unpivot* fra wide til long\n",
    "```python\n",
    "df.melt(id_vars, # kombinasjon av kolonne som unik identifiserer obs\n",
    "        value_vars, # bruker alle som ikke er i id_vars som default.. kan spesifisere for å droppe resten..\n",
    "        var_name, # navn på kolonnen med verdier fra opprinnelige kolonner\n",
    "        value_name) # navn på kolonnen med verdi som korresponedere med opprinelig kolonner\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bruker hvis hver rekke korresponderer med mer enn én observasjon. Hva om hver rekke korresponderer med en dato og har observasjon for flere enheter på hver dato. For å gjøre tidy vil vi én observasjon per enhet per dato.\n",
    "```python\n",
    "df = pd.DataFrame({'tid': {0: 'a', 1: 'b', 2: 'c'},\n",
    "                   'obs_A': {0: 1, 1: 3, 2: 5},\n",
    "                   'obs_B': {0: 2, 1: 4, 2: 6}})\n",
    "df.melt(id_vars='tid',value_vars=['obs_A','obs_B'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tror jeg kan gjenskape atferd i pivot og melt med `stack`, `unstack`, `set_index` og `reset_index`. Trenger da ikke å bruke high-level convinence funksjoner, og trenger derfor ikke huske atferden deres.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legger til nytt nivå innerst i multiindex og sender kolonnenavnene dit. Alternativ måte å identifisere hvilken identifikator (row/column) som korresponderer med hvilken celle. Returnerer Series siden vi ikke lenger har kolonner (såfremt det ikke var multiindex i kolonnene)\n",
    "```python\n",
    "df = pd.DataFrame([[0, 1], [2, 3]],index=['cat', 'dog'], columns=['weight', 'height'])\n",
    "df.stack()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den inverse operasjonen der vi tar de unike verdiene fra innerste nivå av multiindex og bruker de til å konstruere kolonneindex. Returnerer DataFrame. Trenger ikke være innerste, men det er default.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformasjoner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I praksis er det mange innebygde funksjoner som kjører elementvis by default, så dette er ikke så veldig vesentlig tror jeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vil gjøre funksjon elementvis på rekke. Gir ut array med samme størrelse der output i rekken avhenger av input i rekken. \n",
    "\n",
    "Kan ta dict som argument hvis vi vil omkode verdier i henhold til tabell\n",
    "```python\n",
    "table = {'old_val0':'new_val0', 'old_val1':'new_val1'}\n",
    "df['col'] = df['col'].map(table)\n",
    "```\n",
    "Kan også bruke lambda for å lage custom funksjoner\n",
    "```python\n",
    "df['col'] = df['col'].map(lambda x: 2*x) # x er verdi i rekken av array\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metode til Series og DataFrame. På series caller den funksjon elementvis på innhold. Bruker til å finne andeler av gruppe etter groupby,\n",
    "```python\n",
    "antall = df.groupby('key')['col'].sum() # samlet antall for ulike verdier av 'col' for hver verdi av key\n",
    "andel = antall.groupby(level=0).apply(lambda x: round(x/x.sum(), 2)) # andel av samlet antall for gitt key\n",
    "```\n",
    "der hvert argument i apply er series som korresponderer med ulike verdier av key. Det er litt lite ryddig at jeg må kjøre groupby to ganger siden jeg binder til midlertidig variabel og deretter concat/merger. Vet ikke hva som er best practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applymap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tingen er at .apply() anvender funksjon på hvert element av iterable, men har ikke helt bilde av helheten.\n",
    "\n",
    "Hvis jeg i stedet bruke .pipe() på groupby så vet den hvor mange grupper det er og sånn.\n",
    "\n",
    "hmmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan lage nye kolonne fra ifelse conditional med\n",
    "\n",
    "```python\n",
    "df['new_col'] = np.where(df['old_col']=='value', 'A', 'B') # kan bruke til å omkode fra string til boolean\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kombinere dataframes med joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se SQL for beskrivelse av ulike joins. Tror jeg i praksis vil bruke concat og og merge i stedet for join i pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis har samme kolonner eller samme index i to dataframes så er det enklest å bare slenge de sammen med\n",
    "```python\n",
    "pd.concat([liste av dfs],\n",
    "          axis=0)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bruker verdi i felles kolonner til å merge verdier. Tenk at vi har observasjon om hvilke land individer kommer fra. På bakgrunn av dette vil vi koble på mer informasjon om landet på hvert individ. \n",
    "```python\n",
    "left_df.merge(right_df,\n",
    "              on='key', # hvis kolonnene vi joiner på har samme navn\n",
    "              left_on, right_on, # hvis de har ulike navn\n",
    "              left_index, right_index, # sette True dersom vi bruker index i stedet for kolonnenavn\n",
    "              suffixed = ('_x', '_y') # håndtere overlappende kolonnenavn\n",
    "              ) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tror jeg bruker merge i stedet .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legge til series (rad og kolonne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis vi bruker df.append() så lages nytt objekt. Må sende en series eller dataframe som argument. \n",
    "\n",
    "```python\n",
    "df = dt.append(pd.Series({dict: keys:values})) # keys korresponderer med kolonnenavn\n",
    "```\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avgrense til å betrakte delmengde av rader og rekker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset av kolonner med reindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For å velge delmengde av kolonner bruker vi .reindex()\n",
    "``` python\n",
    "df = df[col_lists] # stygt!\n",
    "df = (df.reindex(columns=col_list).\n",
    "      ... # andre metoder, chaine uten å binde til midlertidige dfs hele tiden. vakkert!\n",
    "     )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan bruke bools til å indexe. Gir ut array som \"bli lagt oppå dataframen\". Får ut de verdiene som korresponderer med True.     \n",
    "\n",
    "```python\n",
    "df_new = df[df['col'] == verdi]\n",
    "df_new = df[(df[\"col1\"] > 0) & (df[\"col2\"] == 0)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ikke ideelt at jeg må binde til ny variabel siden det ryddigere å chaine method calls (som i dplyr), skal derfor se på andre måter å anvende boolean mask. Tror jeg vil bruke query i stedet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eneste argumenter er en string som blir parset av `pd.eval`. Ser litt u-pythonisk ut, men beste alternativ for å filtrere i dplyr-ish workflow\n",
    "```python\n",
    "df = (df.query(\"colname > value\").\n",
    "      query(\"colname1 > colname2\"). # sammenligne verdier i ulike kolonner\n",
    "      query(\"`col name1`==value\"). # backtick for å håndtere kolonnenavn med whitespace\n",
    "      query(\"colname == ['a', 'b']\"). # sjekke om verdi er element i liste\n",
    "      query(\"colname == @some_list\"). # bruker @ til å referere til objekt ekstern objekt i namespace \n",
    "      query(\"colname1 > value & colname2 < value\"). # kan kombinere predikat på vanlig måte\n",
    "     )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diverse data cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksempler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Omdefinere verdier i kolonne i henhold til tabell/dict\n",
    "table = {'Amer-Indian-Eskimo':'Native','Asian-Pac-Islander':'Pacific'}\n",
    "df['race'] = (df['race'].\n",
    "    str.strip().\n",
    "    astype('category').\n",
    "    cat.rename_categories(table))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "# Vil ha kolonnenavn i camel_case i stedet for SnakeCase\n",
    "def camel_to_snake(s):\n",
    "    return ''.join('_'+ch.to_lower() if ch.isupper() else ch for ch in s)\n",
    "df.columns = [camel_to_snake(column) for column in df.columns]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# fjerne alle symboler i string som ikke er numerisk\n",
    "df['name'] = df['name'].map(lambda x: ''.join([ch for ch in x if ch.isdigit()]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pd.cut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når vi vil dele kontinuerlige data inn i bins. Konstruere 'factor' med levels, kan deretter lage dummies\n",
    "\n",
    "```python\n",
    "pd.cut(array,\n",
    "       bins, # Antall bins (lager slik at uniform antall i hver) eller konstruere med lik avstand mellom\n",
    "       retbins=True, # spesifisere om returnere cutoffs til bins\n",
    "       labels # kan bruke np.range(len(bins)) hvis kun interessert i relativ plassering\n",
    "       )\n",
    "```\n",
    "\n",
    "bins kan ta ulike form:\n",
    "1. int, spesifiserer antall bins, partisjonerer (min(x),max(x)) inn i like store intervall\n",
    "2. sekvens av tall (a,b,c,..), lager intervall (a,b],(b,c] hvis right=True. Gir NaN hvis ikke i angitte intervall.\n",
    "3. IntervalIndex, konsturert fra array av Interval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nyttige metoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "s.pct_change() # hver observasjon fra i=1 -> n er erstattet med df[i]/df[i-1]-1. Gang med 100 for %\n",
    "s.nlargest() # gir series med index og verdi til n største verdiene i en series\n",
    "s.idxmax() # gir første index til høyeste verdi langs gitt index.. hvis axis=1 så får vi kolonnen med høyest verdi \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det er ganske greit å rekonstruere en kategorisk variabel fra dummies (f.eks for plotting purposes eller hvis jeg vil kjøre groupby på det). Kategorisk er egentlig alltid en bedre representasjon før vi kjører det inn i algoritme, så synes egentlig ikke jeg bør se noen fuckings dummies okay det dritet der bør det være mulig å skjule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan bruke list comprehension til å finne subset kolonner med gitte egenskap\n",
    "```python\n",
    "subset_cols = [col for col in df.columns if col.startswidth('prefix')]\n",
    "subset_df = df[subset_cols]\n",
    "\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "num_cols = [col for col in df.columns if is_numeric_dtype(df[col])]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan bruke `dask.dataframe` når vi har datasett som er for stort til å laste hele inn i minnet samtidig. Har api som er veldig analog til pandas dataframe\n",
    "```python\n",
    "import dask.dataframe as dd\n",
    "df = dd.read_csv('filename.csv')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.837px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
